{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZfszNxxEjbLC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6pEkxOIjsf-",
    "outputId": "85e76029-d98f-4460-ec54-caaf08ed4c5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZvU7f-IDr15F",
    "outputId": "28427381-5279-4868-8974-854253e71f10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy==1.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
      "\u001b[K     |████████████████████████████████| 31.2MB 190kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.18.5)\n",
      "\u001b[31mERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.3.0 has requirement scipy==1.4.1, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: scipy\n",
      "  Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "Successfully installed scipy-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gwjumZLxkEbF"
   },
   "outputs": [],
   "source": [
    "import itertools, imageio, torch, random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from scipy.misc import imresize\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "  \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tanaNB1RuJdN"
   },
   "source": [
    "## Config for CartoonGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlURbt8WuIaj"
   },
   "outputs": [],
   "source": [
    "args = {'name': 'sample_data', \n",
    "        'src_data': 'src_data_path', \n",
    "        'tgt_data': 'tgt_data_path', \n",
    "        'vgg_model': 'pre_trained_VGG19_model_path/vgg19.pth', \n",
    "        'in_ngc': 3, \n",
    "        'out_ngc': 3, \n",
    "        'in_ndc': 3, \n",
    "        'out_ndc': 1, \n",
    "        'batch_size': 8, \n",
    "        'ngf': 64, \n",
    "        'ndf': 32, \n",
    "        'nb': 8, \n",
    "        'input_size': 256, \n",
    "        'train_epoch': 30, \n",
    "        'pre_train_epoch': 10, \n",
    "        'lrD': 0.0002, \n",
    "        'lrG': 0.0002, \n",
    "        'con_lambda': 10, \n",
    "        'beta1': 0.5, \n",
    "        'beta2': 0.999, \n",
    "        'latest_generator_model': '', \n",
    "        'latest_discriminator_model': ''}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwsS3wB8o1g9"
   },
   "source": [
    "## Define and Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWhjB5KSjubD"
   },
   "outputs": [],
   "source": [
    "def data_load(path, subfolder, transform, batch_size, shuffle=False, drop_last=True):\n",
    "    dset = datasets.ImageFolder(path, transform)\n",
    "    ind = dset.class_to_idx[subfolder]\n",
    "\n",
    "    n = 0\n",
    "    for i in range(dset.__len__()):\n",
    "        if ind != dset.imgs[n][1]:\n",
    "            del dset.imgs[n]\n",
    "            n -= 1\n",
    "\n",
    "        n += 1\n",
    "\n",
    "    return torch.utils.data.DataLoader(dset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "def print_network(net):\n",
    "    num_params = 0\n",
    "    for param in net.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(net)\n",
    "    print('Total number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zXo1yX0pYMP"
   },
   "outputs": [],
   "source": [
    "# data_loader\n",
    "# input_size is 256x256\n",
    "src_transform = transforms.Compose([\n",
    "        transforms.Resize((args['input_size'], args['input_size'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "tgt_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqK-StdWj0Cy"
   },
   "outputs": [],
   "source": [
    "data_path = '/content/drive/MyDrive/archive/ds_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7xbkqF7o-27"
   },
   "outputs": [],
   "source": [
    "train_loader_src = data_load(os.path.join(data_path, 'src_data'), 'train', src_transform, args['batch_size'], shuffle=True, drop_last=True)\n",
    "train_loader_tgt = data_load(os.path.join(data_path, 'tgt_data'), 'pair', tgt_transform, args['batch_size'], shuffle=True, drop_last=True)\n",
    "test_loader_src = data_load(os.path.join(data_path, 'src_data'), 'test', src_transform, 1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7hGEVfsVJCBu",
    "outputId": "32d941ed-c53d-4304-ab65-6a11863ac8be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2p1ixog0JBcd"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95sFMoS7wgQ5"
   },
   "source": [
    "## Define Generator, Discriminator, and VGG Network(feature extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-fw2hlitnEg"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(net):\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.ConvTranspose2d):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "            \n",
    "class resnet_block(nn.Module):\n",
    "    def __init__(self, channel, kernel, stride, padding):\n",
    "        super(resnet_block, self).__init__()\n",
    "        self.channel = channel\n",
    "        self.kernel = kernel\n",
    "        self.strdie = stride\n",
    "        self.padding = padding\n",
    "        self.conv1 = nn.Conv2d(channel, channel, kernel, stride, padding)\n",
    "        self.conv1_norm = nn.InstanceNorm2d(channel)\n",
    "        self.conv2 = nn.Conv2d(channel, channel, kernel, stride, padding)\n",
    "        self.conv2_norm = nn.InstanceNorm2d(channel)\n",
    "\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = F.relu(self.conv1_norm(self.conv1(input)), True)\n",
    "        x = self.conv2_norm(self.conv2(x))\n",
    "\n",
    "        return input + x #Elementwise Sum\n",
    " \n",
    "\n",
    "class generator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, in_nc, out_nc, nf=32, nb=6):\n",
    "        super(generator, self).__init__()\n",
    "        self.input_nc = in_nc\n",
    "        self.output_nc = out_nc\n",
    "        self.nf = nf\n",
    "        self.nb = nb\n",
    "        self.down_convs = nn.Sequential(\n",
    "            nn.Conv2d(in_nc, nf, 7, 1, 3), #k7n64s1\n",
    "            nn.InstanceNorm2d(nf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(nf, nf * 2, 3, 2, 1), #k3n128s2\n",
    "            nn.Conv2d(nf * 2, nf * 2, 3, 1, 1), #k3n128s1\n",
    "            nn.InstanceNorm2d(nf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(nf * 2, nf * 4, 3, 2, 1), #k3n256s1\n",
    "            nn.Conv2d(nf * 4, nf * 4, 3, 1, 1), #k3n256s1\n",
    "            nn.InstanceNorm2d(nf * 4),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.resnet_blocks = []\n",
    "        for i in range(nb):\n",
    "            self.resnet_blocks.append(resnet_block(nf * 4, 3, 1, 1))\n",
    "\n",
    "        self.resnet_blocks = nn.Sequential(*self.resnet_blocks)\n",
    "\n",
    "        self.up_convs = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nf * 4, nf * 2, 3, 2, 1, 1), #k3n128s1/2\n",
    "            nn.Conv2d(nf * 2, nf * 2, 3, 1, 1), #k3n128s1\n",
    "            nn.InstanceNorm2d(nf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(nf * 2, nf, 3, 2, 1, 1), #k3n64s1/2\n",
    "            nn.Conv2d(nf, nf, 3, 1, 1), #k3n64s1\n",
    "            nn.InstanceNorm2d(nf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(nf, out_nc, 7, 1, 3), #k7n3s1\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        initialize_weights(self)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        x = self.down_convs(input)\n",
    "        x = self.resnet_blocks(x)\n",
    "        output = self.up_convs(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, in_nc, out_nc, nf=32):\n",
    "        super(discriminator, self).__init__()\n",
    "        self.input_nc = in_nc\n",
    "        self.output_nc = out_nc\n",
    "        self.nf = nf\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(in_nc, nf, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(nf, nf * 2, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(nf * 2, nf * 4, 3, 1, 1),\n",
    "            nn.InstanceNorm2d(nf * 4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(nf * 4, nf * 4, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(nf * 4, nf * 8, 3, 1, 1),\n",
    "            nn.InstanceNorm2d(nf * 8),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(nf * 8, nf * 8, 3, 1, 1),\n",
    "            nn.InstanceNorm2d(nf * 8),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(nf * 8, out_nc, 3, 1, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        initialize_weights(self)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        # input = torch.cat((input1, input2), 1)\n",
    "        output = self.convs(input)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class VGG19(nn.Module):\n",
    "    def __init__(self, init_weights=None, feature_mode=False, batch_norm=False, num_classes=1000):\n",
    "        super(VGG19, self).__init__()\n",
    "        self.cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n",
    "        self.init_weights = init_weights\n",
    "        self.feature_mode = feature_mode\n",
    "        self.batch_norm = batch_norm\n",
    "        self.num_clases = num_classes\n",
    "        self.features = self.make_layers(self.cfg, batch_norm)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        if not init_weights == None:\n",
    "            self.load_state_dict(torch.load(init_weights))\n",
    "\n",
    "    def make_layers(self, cfg, batch_norm=False):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.feature_mode:\n",
    "            module_list = list(self.features.modules())\n",
    "            for l in module_list[1:27]:                 # conv4_4\n",
    "                x = l(x)\n",
    "        if not self.feature_mode:\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SD8NgT8NxFrN"
   },
   "source": [
    "## Load Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLE13LClw5a3"
   },
   "outputs": [],
   "source": [
    "Gen = generator(args['in_ngc'], args['out_ngc'], args['ngf'], args['nb'])\n",
    "Dis = discriminator(args['in_ndc'], args['out_ndc'], args['ndf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8FtGV91xwu6"
   },
   "source": [
    "## Load Pre trained VGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "khvNAUf60TFV",
    "outputId": "c84f98e1-fca9-4a20-c627-2cd046373cd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vgg19-dcbb9e9d.pth', 'src_data', 'tgt_data', 'generator_latest.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CcCUn4Ih0NIN"
   },
   "outputs": [],
   "source": [
    "vgg_model_path = os.path.join(data_path,'vgg19-dcbb9e9d.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlCNZtOwxP6g"
   },
   "outputs": [],
   "source": [
    "VGG = VGG19(init_weights=vgg_model_path, feature_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_Qx9-jg1kz3"
   },
   "source": [
    "## Device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oUIRVHMp1joC",
    "outputId": "ad51aa32-bfa8-46a2-9efa-c1158731fd1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Networks initialized -------------\n",
      "generator(\n",
      "  (down_convs): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (10): ReLU(inplace=True)\n",
      "  )\n",
      "  (resnet_blocks): Sequential(\n",
      "    (0): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (1): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (2): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (3): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (4): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (5): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (6): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (7): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "  )\n",
      "  (up_convs): Sequential(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "    (9): Tanh()\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 11120195\n",
      "discriminator(\n",
      "  (convs): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (10): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (11): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (14): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (15): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 1128385\n",
      "VGG19(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 143667240\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "Gen.to(device)\n",
    "Dis.to(device)\n",
    "VGG.to(device)\n",
    "Gen.train()\n",
    "Dis.train()\n",
    "VGG.eval()\n",
    "print('---------- Networks initialized -------------')\n",
    "print_network(Gen)\n",
    "print_network(Dis)\n",
    "print_network(VGG)\n",
    "print('-----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcOyZhM90STl"
   },
   "outputs": [],
   "source": [
    "# optimizers\n",
    "BCE_loss = nn.BCELoss().to(device)\n",
    "L1_loss = nn.L1Loss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYhvy0WA2UdL"
   },
   "outputs": [],
   "source": [
    "G_optimizer = optim.Adam(Gen.parameters(), lr=args['lrG'], betas=(args['beta1'], args['beta2']))\n",
    "D_optimizer = optim.Adam(Dis.parameters(), lr=args['lrD'], betas=(args['beta1'], args['beta2']))\n",
    "G_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=G_optimizer, milestones=[args['train_epoch'] // 2, \n",
    "                                                                                args['train_epoch'] // 4 * 3], \n",
    "                                                                                gamma=0.1)\n",
    "D_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=D_optimizer, milestones=[args['train_epoch'] // 2, \n",
    "                                                                                args['train_epoch'] // 4 * 3], gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1GUu3Nq3oH-"
   },
   "source": [
    "### Pre-training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMQ-YuG6__pt",
    "outputId": "73ac121e-6e0b-4af7-9ac5-39b5aee15d51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anscombe.json',\n",
       " 'README.md',\n",
       " 'mnist_test.csv',\n",
       " 'california_housing_train.csv',\n",
       " 'mnist_train_small.csv',\n",
       " 'california_housing_test.csv']"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_path = './sample_data'\n",
    "os.listdir(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbxHwGUh97mf"
   },
   "outputs": [],
   "source": [
    "# path for saving results\n",
    "if not os.path.isdir(os.path.join(args['name'] + '_results', 'Reconstruction')):\n",
    "    os.makedirs(os.path.join(args['name'] + '_results', 'Reconstruction'))\n",
    "\n",
    "if not os.path.isdir(os.path.join(args['name'] + '_results', 'Transfer')):\n",
    "    os.makedirs(os.path.join(args['name'] + '_results', 'Transfer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2q2qTYi827OK",
    "outputId": "0f7d4eb3-89e8-468e-86f2-2c9dc100d68e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training start!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] - time: 1541.07, Recon loss: 18.384\n",
      "[2/10] - time: 173.42, Recon loss: 8.683\n",
      "[3/10] - time: 171.74, Recon loss: 6.747\n",
      "[4/10] - time: 171.90, Recon loss: 5.731\n",
      "[5/10] - time: 171.60, Recon loss: 5.082\n",
      "[6/10] - time: 172.22, Recon loss: 4.652\n",
      "[7/10] - time: 172.08, Recon loss: 4.350\n",
      "[8/10] - time: 171.45, Recon loss: 4.106\n",
      "[9/10] - time: 171.35, Recon loss: 3.935\n",
      "[10/10] - time: 171.46, Recon loss: 3.707\n"
     ]
    }
   ],
   "source": [
    "pre_train_hist = {}\n",
    "pre_train_hist['Recon_loss'] = []\n",
    "pre_train_hist['per_epoch_time'] = []\n",
    "pre_train_hist['total_time'] = []\n",
    "\n",
    "print('Pre-training start!')\n",
    "start_time = time.time()\n",
    "for epoch in range(args['pre_train_epoch']):\n",
    "    epoch_start_time = time.time()\n",
    "    Recon_losses = []\n",
    "\n",
    "    for x, _ in train_loader_src:\n",
    "        x = x.to(device)\n",
    "\n",
    "        # train generator G\n",
    "        G_optimizer.zero_grad()\n",
    "\n",
    "        x_feature = VGG((x + 1) / 2)\n",
    "        G_ = Gen(x)\n",
    "        G_feature = VGG((G_ + 1) / 2)\n",
    "\n",
    "        Recon_loss = 10 * L1_loss(G_feature, x_feature.detach())\n",
    "        Recon_losses.append(Recon_loss.item())\n",
    "        pre_train_hist['Recon_loss'].append(Recon_loss.item())\n",
    "\n",
    "        Recon_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "    per_epoch_time = time.time() - epoch_start_time\n",
    "    pre_train_hist['per_epoch_time'].append(per_epoch_time)\n",
    "    print('[%d/%d] - time: %.2f, Recon loss: %.3f' % ((epoch + 1), \n",
    "                                                      args['pre_train_epoch'], \n",
    "                                                      per_epoch_time, \n",
    "                                                      torch.mean(torch.FloatTensor(Recon_losses))))\n",
    "    \n",
    "    torch.save(Gen.state_dict(), os.path.join(args['name'] + '_results', 'generator_latest.pkl'))\n",
    "            \n",
    "total_time = time.time() - start_time\n",
    "# pre_train_hist['total_time'].append(total_time)\n",
    "# with open(os.path.join(args['name'] + '_results',  'pre_train_hist.pkl'), 'wb') as f:\n",
    "#     pickle.dump(pre_train_hist, f)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     Gen.eval()\n",
    "#     for n, (x, _) in enumerate(train_loader_src):\n",
    "#         x = x.to(device)\n",
    "#         G_recon = Gen(x)\n",
    "#         result = torch.cat((x[0], G_recon[0]), 2)\n",
    "#         path = os.path.join(args['name'] + '_results', 'Reconstruction', args['name'] + '_train_recon_' + str(n + 1) + '.png')\n",
    "#         plt.imsave(path, (result.cpu().numpy().transpose(1, 2, 0) + 1) / 2)\n",
    "#         if n == 4:\n",
    "#             break\n",
    "\n",
    "#     for n, (x, _) in enumerate(test_loader_src):\n",
    "#         x = x.to(device)\n",
    "#         G_recon = Gen(x)\n",
    "#         result = torch.cat((x[0], G_recon[0]), 2)\n",
    "#         path = os.path.join(args['name'] + '_results', 'Reconstruction', args['name'] + '_test_recon_' + str(n + 1) + '.png')\n",
    "#         plt.imsave(path, (result.cpu().numpy().transpose(1, 2, 0) + 1) / 2)\n",
    "#         if n == 4:\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yZI2RS2Z1BS"
   },
   "outputs": [],
   "source": [
    "torch.save(Gen.state_dict(), os.path.join(data_path, 'generator_latest.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbdoYyqXV6g2"
   },
   "outputs": [],
   "source": [
    "pre_train_hist['total_time'].append(total_time)\n",
    "with open(os.path.join(args['name'] + '_results',  'pre_train_hist.pkl'), 'wb') as f:\n",
    "    pickle.dump(pre_train_hist, f)\n",
    "\n",
    "with torch.no_grad():\n",
    "    Gen.eval()\n",
    "    for n, (x, _) in enumerate(train_loader_src):\n",
    "        x = x.to(device)\n",
    "        G_recon = Gen(x)\n",
    "        result = torch.cat((x[0], G_recon[0]), 2)\n",
    "        path = os.path.join(args['name'] + '_results', 'Reconstruction', args['name'] + '_train_recon_' + str(n + 1) + '.png')\n",
    "        plt.imsave(path, (result.cpu().numpy().transpose(1, 2, 0) + 1) / 2)\n",
    "        if n == 4:\n",
    "            break\n",
    "\n",
    "    for n, (x, _) in enumerate(test_loader_src):\n",
    "        x = x.to(device)\n",
    "        G_recon = Gen(x)\n",
    "        result = torch.cat((x[0], G_recon[0]), 2)\n",
    "        path = os.path.join(args['name'] + '_results', 'Reconstruction', args['name'] + '_test_recon_' + str(n + 1) + '.png')\n",
    "        plt.imsave(path, (result.cpu().numpy().transpose(1, 2, 0) + 1) / 2)\n",
    "        if n == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU1D_TJhWkq7"
   },
   "source": [
    "## Training Generator and Discriminator together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fv05ynULWkBn",
    "outputId": "71ce7262-7880-4dcc-9be0-61d8a8b0ab81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] - time: 1592.92, Disc loss: 1.745, Gen loss: 1.671, Con loss: 3.734\n",
      "[2/100] - time: 294.88, Disc loss: 1.564, Gen loss: 2.366, Con loss: 3.882\n",
      "[3/100] - time: 291.91, Disc loss: 1.461, Gen loss: 2.603, Con loss: 3.939\n",
      "[4/100] - time: 290.91, Disc loss: 1.234, Gen loss: 2.735, Con loss: 4.104\n",
      "[5/100] - time: 291.80, Disc loss: 0.769, Gen loss: 3.238, Con loss: 4.944\n",
      "[6/100] - time: 291.47, Disc loss: 0.628, Gen loss: 3.612, Con loss: 5.821\n",
      "[7/100] - time: 290.61, Disc loss: 0.565, Gen loss: 3.768, Con loss: 6.510\n",
      "[8/100] - time: 289.23, Disc loss: 0.513, Gen loss: 3.869, Con loss: 7.085\n",
      "[9/100] - time: 290.33, Disc loss: 0.504, Gen loss: 3.852, Con loss: 7.468\n",
      "[10/100] - time: 291.33, Disc loss: 0.569, Gen loss: 3.517, Con loss: 7.588\n",
      "[11/100] - time: 291.56, Disc loss: 0.494, Gen loss: 3.541, Con loss: 7.999\n",
      "[12/100] - time: 291.56, Disc loss: 0.493, Gen loss: 3.565, Con loss: 8.272\n",
      "[13/100] - time: 291.39, Disc loss: 0.707, Gen loss: 3.278, Con loss: 7.617\n",
      "[14/100] - time: 291.28, Disc loss: 0.420, Gen loss: 3.593, Con loss: 7.560\n",
      "[15/100] - time: 292.42, Disc loss: 0.429, Gen loss: 3.680, Con loss: 7.956\n",
      "[16/100] - time: 292.08, Disc loss: 0.222, Gen loss: 4.849, Con loss: 7.373\n",
      "[17/100] - time: 290.70, Disc loss: 0.033, Gen loss: 6.351, Con loss: 5.894\n",
      "[18/100] - time: 291.82, Disc loss: 0.553, Gen loss: 4.544, Con loss: 6.689\n",
      "[19/100] - time: 292.19, Disc loss: 0.435, Gen loss: 3.847, Con loss: 8.007\n",
      "[20/100] - time: 292.42, Disc loss: 0.370, Gen loss: 3.929, Con loss: 8.074\n",
      "[21/100] - time: 293.05, Disc loss: 0.554, Gen loss: 3.757, Con loss: 7.704\n",
      "[22/100] - time: 292.08, Disc loss: 0.315, Gen loss: 4.161, Con loss: 7.959\n",
      "[23/100] - time: 291.51, Disc loss: 0.414, Gen loss: 3.992, Con loss: 7.792\n",
      "[24/100] - time: 291.69, Disc loss: 0.334, Gen loss: 4.242, Con loss: 7.972\n",
      "[25/100] - time: 291.34, Disc loss: 0.324, Gen loss: 4.265, Con loss: 7.935\n",
      "[26/100] - time: 290.92, Disc loss: 0.301, Gen loss: 4.550, Con loss: 7.817\n",
      "[27/100] - time: 289.74, Disc loss: 0.269, Gen loss: 4.794, Con loss: 7.539\n",
      "[28/100] - time: 289.15, Disc loss: 0.286, Gen loss: 4.752, Con loss: 7.572\n",
      "[29/100] - time: 289.74, Disc loss: 0.613, Gen loss: 4.136, Con loss: 6.970\n",
      "[30/100] - time: 290.31, Disc loss: 0.259, Gen loss: 4.653, Con loss: 7.245\n",
      "[31/100] - time: 290.93, Disc loss: 0.252, Gen loss: 4.941, Con loss: 7.161\n",
      "[32/100] - time: 290.81, Disc loss: 0.228, Gen loss: 5.147, Con loss: 6.975\n",
      "[33/100] - time: 290.73, Disc loss: 0.175, Gen loss: 5.724, Con loss: 6.527\n",
      "[34/100] - time: 291.43, Disc loss: 0.507, Gen loss: 4.774, Con loss: 6.286\n",
      "[35/100] - time: 291.16, Disc loss: 0.152, Gen loss: 5.794, Con loss: 6.092\n",
      "[36/100] - time: 289.46, Disc loss: 0.019, Gen loss: 6.586, Con loss: 5.786\n",
      "[37/100] - time: 291.93, Disc loss: 0.291, Gen loss: 7.035, Con loss: 6.826\n",
      "[38/100] - time: 292.06, Disc loss: 0.277, Gen loss: 5.090, Con loss: 5.596\n",
      "[39/100] - time: 291.56, Disc loss: 0.113, Gen loss: 6.367, Con loss: 5.277\n",
      "[40/100] - time: 291.76, Disc loss: 0.156, Gen loss: 6.357, Con loss: 5.620\n",
      "[41/100] - time: 290.51, Disc loss: 0.112, Gen loss: 6.622, Con loss: 5.396\n",
      "[42/100] - time: 290.55, Disc loss: 0.107, Gen loss: 7.041, Con loss: 4.860\n",
      "[43/100] - time: 288.62, Disc loss: 0.128, Gen loss: 6.722, Con loss: 5.172\n",
      "[44/100] - time: 287.72, Disc loss: 0.724, Gen loss: 5.751, Con loss: 4.280\n",
      "[45/100] - time: 288.42, Disc loss: 0.657, Gen loss: 3.882, Con loss: 4.034\n",
      "[46/100] - time: 287.92, Disc loss: 0.053, Gen loss: 6.398, Con loss: 3.954\n",
      "[47/100] - time: 286.23, Disc loss: 0.076, Gen loss: 6.685, Con loss: 4.161\n",
      "[48/100] - time: 286.61, Disc loss: 0.021, Gen loss: 6.978, Con loss: 4.197\n",
      "[49/100] - time: 286.98, Disc loss: 0.008, Gen loss: 6.782, Con loss: 5.022\n",
      "[50/100] - time: 286.19, Disc loss: 0.005, Gen loss: 6.730, Con loss: 5.218\n",
      "[51/100] - time: 285.83, Disc loss: 0.005, Gen loss: 6.788, Con loss: 5.288\n",
      "[52/100] - time: 284.00, Disc loss: 0.004, Gen loss: 6.975, Con loss: 5.387\n",
      "[53/100] - time: 285.18, Disc loss: 0.004, Gen loss: 7.091, Con loss: 5.657\n",
      "[54/100] - time: 284.20, Disc loss: 0.003, Gen loss: 7.428, Con loss: 5.508\n",
      "[55/100] - time: 285.65, Disc loss: 0.003, Gen loss: 7.435, Con loss: 5.329\n",
      "[56/100] - time: 287.09, Disc loss: 0.003, Gen loss: 7.820, Con loss: 5.359\n",
      "[57/100] - time: 288.29, Disc loss: 0.003, Gen loss: 7.840, Con loss: 5.248\n",
      "[58/100] - time: 289.11, Disc loss: 0.004, Gen loss: 7.787, Con loss: 5.817\n",
      "[59/100] - time: 289.35, Disc loss: 0.004, Gen loss: 7.843, Con loss: 6.291\n",
      "[60/100] - time: 288.11, Disc loss: 0.018, Gen loss: 7.412, Con loss: 9.635\n",
      "[61/100] - time: 286.30, Disc loss: 0.080, Gen loss: 5.420, Con loss: 12.446\n",
      "[62/100] - time: 285.26, Disc loss: 0.100, Gen loss: 4.833, Con loss: 12.148\n",
      "[63/100] - time: 283.64, Disc loss: 0.113, Gen loss: 4.857, Con loss: 11.680\n",
      "[64/100] - time: 284.37, Disc loss: 0.096, Gen loss: 5.447, Con loss: 10.239\n",
      "[65/100] - time: 283.56, Disc loss: 0.070, Gen loss: 6.394, Con loss: 8.220\n",
      "[66/100] - time: 283.25, Disc loss: 0.024, Gen loss: 7.124, Con loss: 5.876\n",
      "[67/100] - time: 283.72, Disc loss: 0.008, Gen loss: 7.408, Con loss: 5.540\n",
      "[68/100] - time: 283.09, Disc loss: 0.012, Gen loss: 7.778, Con loss: 5.607\n",
      "[69/100] - time: 282.31, Disc loss: 0.012, Gen loss: 8.207, Con loss: 5.077\n",
      "[70/100] - time: 281.63, Disc loss: 0.014, Gen loss: 8.304, Con loss: 4.775\n",
      "[71/100] - time: 281.08, Disc loss: 0.009, Gen loss: 8.308, Con loss: 4.379\n",
      "[72/100] - time: 281.08, Disc loss: 0.004, Gen loss: 7.555, Con loss: 4.443\n",
      "[73/100] - time: 282.19, Disc loss: 0.003, Gen loss: 7.707, Con loss: 4.734\n",
      "[74/100] - time: 282.21, Disc loss: 0.003, Gen loss: 7.845, Con loss: 4.925\n",
      "[75/100] - time: 281.73, Disc loss: 0.003, Gen loss: 7.892, Con loss: 5.028\n",
      "[76/100] - time: 284.44, Disc loss: 0.002, Gen loss: 7.906, Con loss: 5.033\n",
      "[77/100] - time: 283.42, Disc loss: 0.002, Gen loss: 8.039, Con loss: 5.033\n",
      "[78/100] - time: 283.71, Disc loss: 0.002, Gen loss: 8.167, Con loss: 5.033\n",
      "[79/100] - time: 286.54, Disc loss: 0.002, Gen loss: 8.268, Con loss: 5.031\n",
      "[80/100] - time: 287.08, Disc loss: 0.001, Gen loss: 8.366, Con loss: 5.042\n",
      "[81/100] - time: 289.33, Disc loss: 0.001, Gen loss: 8.446, Con loss: 5.063\n",
      "[82/100] - time: 288.31, Disc loss: 0.001, Gen loss: 8.481, Con loss: 5.097\n",
      "[83/100] - time: 288.96, Disc loss: 0.001, Gen loss: 8.530, Con loss: 5.151\n",
      "[84/100] - time: 287.77, Disc loss: 0.001, Gen loss: 8.599, Con loss: 5.201\n",
      "[85/100] - time: 288.25, Disc loss: 0.001, Gen loss: 8.659, Con loss: 5.256\n",
      "[86/100] - time: 289.76, Disc loss: 0.001, Gen loss: 8.214, Con loss: 5.436\n",
      "[87/100] - time: 290.90, Disc loss: 0.001, Gen loss: 8.594, Con loss: 5.525\n",
      "[88/100] - time: 291.64, Disc loss: 0.001, Gen loss: 8.825, Con loss: 5.566\n",
      "[89/100] - time: 288.41, Disc loss: 0.001, Gen loss: 9.032, Con loss: 5.567\n",
      "[90/100] - time: 290.08, Disc loss: 0.001, Gen loss: 9.261, Con loss: 5.578\n",
      "[91/100] - time: 290.07, Disc loss: 0.001, Gen loss: 9.350, Con loss: 5.588\n",
      "[92/100] - time: 289.41, Disc loss: 0.001, Gen loss: 9.367, Con loss: 5.653\n",
      "[93/100] - time: 289.55, Disc loss: 0.001, Gen loss: 9.189, Con loss: 5.784\n",
      "[94/100] - time: 286.12, Disc loss: 0.001, Gen loss: 8.982, Con loss: 6.099\n",
      "[95/100] - time: 286.33, Disc loss: 0.001, Gen loss: 9.234, Con loss: 6.093\n",
      "[96/100] - time: 284.58, Disc loss: 0.001, Gen loss: 9.336, Con loss: 6.031\n",
      "[97/100] - time: 284.53, Disc loss: 0.001, Gen loss: 9.555, Con loss: 6.004\n",
      "[98/100] - time: 282.61, Disc loss: 0.001, Gen loss: 9.789, Con loss: 5.898\n",
      "[99/100] - time: 284.66, Disc loss: 0.001, Gen loss: 9.580, Con loss: 5.836\n",
      "[100/100] - time: 284.29, Disc loss: 0.001, Gen loss: 8.654, Con loss: 6.021\n"
     ]
    }
   ],
   "source": [
    "train_hist = {}\n",
    "train_hist['Disc_loss'] = []\n",
    "train_hist['Gen_loss'] = []\n",
    "train_hist['Con_loss'] = []\n",
    "train_hist['per_epoch_time'] = []\n",
    "train_hist['total_time'] = []\n",
    "print('training start!')\n",
    "start_time = time.time()\n",
    "real = torch.ones(args['batch_size'], 1, args['input_size'] // 4, args['input_size'] // 4).to(device)\n",
    "fake = torch.zeros(args['batch_size'], 1, args['input_size'] // 4, args['input_size'] // 4).to(device)\n",
    "for epoch in range(args['train_epoch']):\n",
    "    epoch_start_time = time.time()\n",
    "    Gen.train()\n",
    "    G_scheduler.step()\n",
    "    D_scheduler.step()\n",
    "    Disc_losses = []\n",
    "    Gen_losses = []\n",
    "    Con_losses = []\n",
    "    for (x, _), (y, _) in zip(train_loader_src, train_loader_tgt):\n",
    "        e = y[:, :, :, args['input_size']:]\n",
    "        y = y[:, :, :, :args['input_size']]\n",
    "        x, y, e = x.to(device), y.to(device), e.to(device)\n",
    "\n",
    "        # train D\n",
    "        D_optimizer.zero_grad()\n",
    "\n",
    "        D_real = Dis(y)\n",
    "        D_real_loss = BCE_loss(D_real, real)\n",
    "\n",
    "        G_ = Gen(x)\n",
    "        D_fake = Dis(G_)\n",
    "        D_fake_loss = BCE_loss(D_fake, fake)\n",
    "\n",
    "        D_edge = Dis(e)\n",
    "        D_edge_loss = BCE_loss(D_edge, fake)\n",
    "\n",
    "        Disc_loss = D_real_loss + D_fake_loss + D_edge_loss\n",
    "        Disc_losses.append(Disc_loss.item())\n",
    "        train_hist['Disc_loss'].append(Disc_loss.item())\n",
    "\n",
    "        Disc_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # train G\n",
    "        G_optimizer.zero_grad()\n",
    "\n",
    "        G_ = Gen(x)\n",
    "        D_fake = Dis(G_)\n",
    "        D_fake_loss = BCE_loss(D_fake, real)\n",
    "\n",
    "        x_feature = VGG((x + 1) / 2)\n",
    "        G_feature = VGG((G_ + 1) / 2)\n",
    "        Con_loss = args['con_lambda'] * L1_loss(G_feature, x_feature.detach())\n",
    "\n",
    "        Gen_loss = D_fake_loss + Con_loss\n",
    "        Gen_losses.append(D_fake_loss.item())\n",
    "        train_hist['Gen_loss'].append(D_fake_loss.item())\n",
    "        Con_losses.append(Con_loss.item())\n",
    "        train_hist['Con_loss'].append(Con_loss.item())\n",
    "\n",
    "        Gen_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "\n",
    "    per_epoch_time = time.time() - epoch_start_time\n",
    "    train_hist['per_epoch_time'].append(per_epoch_time)\n",
    "    print(\n",
    "    '[%d/%d] - time: %.2f, Disc loss: %.3f, Gen loss: %.3f, Con loss: %.3f' % ((epoch + 1), \n",
    "        args['train_epoch'], per_epoch_time, torch.mean(torch.FloatTensor(Disc_losses)),\n",
    "        torch.mean(torch.FloatTensor(Gen_losses)), torch.mean(torch.FloatTensor(Con_losses))))\n",
    "\n",
    "    if epoch % 2 == 1 or epoch == args['train_epoch'] - 1:\n",
    "        with torch.no_grad():\n",
    "            Gen.eval()\n",
    "            for n, (x, _) in enumerate(train_loader_src):\n",
    "                x = x.to(device)\n",
    "                G_recon = Gen(x)\n",
    "                result = torch.cat((x[0], G_recon[0]), 2)\n",
    "                path = os.path.join(args['name'] + '_results', 'Transfer', str(epoch+1) + '_epoch_' + args['name'] + '_train_' + str(n + 1) + '.png')\n",
    "                plt.imsave(path, (result.cpu().numpy().transpose(1, 2, 0) + 1) / 2)\n",
    "                if n == 4:\n",
    "                    break\n",
    "\n",
    "            for n, (x, _) in enumerate(test_loader_src):\n",
    "                x = x.to(device)\n",
    "                G_recon = Gen(x)\n",
    "                result = torch.cat((x[0], G_recon[0]), 2)\n",
    "                path = os.path.join(args['name'] + '_results', 'Transfer', str(epoch+1) + '_epoch_' + args['name'] + '_test_' + str(n + 1) + '.png')\n",
    "                plt.imsave(path, (result.cpu().numpy().transpose(1, 2, 0) + 1) / 2)\n",
    "                if n == 4:\n",
    "                    break\n",
    "\n",
    "            torch.save(Gen.state_dict(), os.path.join(args['name'] + '_results', 'generator_latest.pkl'))\n",
    "            torch.save(Dis.state_dict(), os.path.join(args['name'] + '_results', 'discriminator_latest.pkl'))\n",
    "            torch.save(Gen.state_dict(), os.path.join(data_path, 'generator_latest.pkl'))\n",
    "            torch.save(Dis.state_dict(), os.path.join(data_path, 'discriminator_latest.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNpgW_XdBsZM"
   },
   "outputs": [],
   "source": [
    "total_time = time.time() - start_time\n",
    "train_hist['total_time'].append(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l3jMuoUzDEvc",
    "outputId": "0a5f7b30-9346-4ff3-984a-dce6c6ea8832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg one epoch time: 301.27, total 100 epochs time: 30302.76\n",
      "Training finish!... save training results\n"
     ]
    }
   ],
   "source": [
    "print(\"Avg one epoch time: %.2f, total %d epochs time: %.2f\" % (torch.mean(torch.FloatTensor(train_hist['per_epoch_time'])), args['train_epoch'], total_time))\n",
    "print(\"Training finish!... save training results\")\n",
    "\n",
    "torch.save(Gen.state_dict(), os.path.join(args['name'] + '_results',  'generator_param.pkl'))\n",
    "torch.save(Dis.state_dict(), os.path.join(args['name'] + '_results',  'discriminator_param.pkl'))\n",
    "with open(os.path.join(args['name'] + '_results',  'train_hist.pkl'), 'wb') as f:\n",
    "    pickle.dump(train_hist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwevoAGnhezR"
   },
   "outputs": [],
   "source": [
    "!cp -r './sample_data_results' '/content/drive/MyDrive/archive/ds_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CoI8XB9Gkdat"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "cartoon-gans-35-epochs",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
